{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce89602",
   "metadata": {},
   "source": [
    "## Задание 1 (6 баллов + 2 доп балла).\n",
    "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
    "\n",
    "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58135cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_en = './opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-train.en'\n",
    "path_uk = './opus-100-corpus/v1.0/supervised/en-uk/opus.en-uk-train.uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b55fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open(path_en).read().splitlines()\n",
    "uk_sents = open(path_uk).read().replace('\\xa0', ' ').splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2722a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Consistant with our other victims.', 'Как и все остальные жертвы мясника.'),\n",
       " ('The children.', 'Діти.'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(en_sents[0], uk_sents[0]), (en_sents[1], uk_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88e393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer_en.train(files=[path_en], trainer=trainer_en)\n",
    "\n",
    "tokenizer_uk = Tokenizer(BPE())\n",
    "tokenizer_uk.pre_tokenizer = Whitespace()\n",
    "trainer_uk = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer_uk.train(files=[path_uk], trainer=trainer_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8bffbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_uk.save('tokenizer_uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "72e2ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_uk = Tokenizer.from_file(\"tokenizer_uk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5e63e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3de9f566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "PAD_IDX = tokenizer_uk.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34d6d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_en, max_len_uk = 50, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c1ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
    "X_uk = [encode(t, tokenizer_uk, max_len_uk) for t in uk_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f96279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1865325/1698114856.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(X_en).shape, np.array(X_uk).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000000,), (1000000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_en).shape, np.array(X_uk).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04723101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts_en, texts_uk):\n",
    "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_IDX)\n",
    "        \n",
    "        self.texts_uk = [torch.LongTensor(sent) for sent in texts_uk]\n",
    "        self.texts_uk = torch.nn.utils.rnn.pad_sequence(self.texts_uk, padding_value=PAD_IDX)\n",
    "\n",
    "        self.length = len(texts_en)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ids_en = self.texts_en[:, index]\n",
    "        ids_uk = self.texts_uk[:, index]\n",
    "\n",
    "        return ids_en, ids_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_uk_train, X_uk_valid = train_test_split(X_en, X_uk, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "100ae51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en_train, X_uk_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True)\n",
    "\n",
    "valid_set = Dataset(X_en_valid, X_uk_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "638e15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "DEVICE = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 150):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size, \n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "#         print('pos inp')\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "#         print('pos dec')\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "#         print('pos out')\n",
    "        x = self.generator(outs)\n",
    "#         print('gen')\n",
    "        return x\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8791a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, iterator, optimizer, criterion, print_every=500):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    ac = []\n",
    "    \n",
    "    model.train()  \n",
    "\n",
    "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "        texts_en = texts_en.T.to(DEVICE) # чтобы батч был в конце\n",
    "        texts_ru = texts_ru.T.to(DEVICE) # чтобы батч был в конце\n",
    "        \n",
    "        # помимо текста в модель еще нужно передать целевую последовательность\n",
    "        # но не полную а без 1 последнего элемента\n",
    "        # а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
    "        texts_ru_input = texts_ru[:-1, :]\n",
    "        \n",
    "        \n",
    "        # в трансформерах нет циклов как в лстм \n",
    "        # каждый элемент связан с каждым через аттеншен\n",
    "        # чтобы имитировать последовательную обработку\n",
    "        # и чтобы не считать аттеншн с паддингом \n",
    "        # в трансформерах нужно считать много масок\n",
    "        # подробнее про это по ссылкам выше\n",
    "        (texts_en_mask, texts_ru_mask, \n",
    "        texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
    "        logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
    "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
    "        texts_ru_out = texts_ru[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "        \n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_f1 = []\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "            texts_en = texts_en.T.to(DEVICE)\n",
    "            texts_ru = texts_ru.T.to(DEVICE)\n",
    "\n",
    "            texts_ru_input = texts_ru[:-1, :]\n",
    "\n",
    "            (texts_en_mask, texts_ru_mask, \n",
    "            texts_en_padding_mask, texts_ru_padding_mask) = create_mask(texts_en, texts_ru_input)\n",
    "\n",
    "            logits = model(texts_en, texts_ru_input, texts_en_mask, texts_ru_mask,\n",
    "                           texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "\n",
    "            \n",
    "            texts_ru_out = texts_ru[1:, :]\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_ru_out.reshape(-1))\n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36288fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
    "UK_VOCAB_SIZE = tokenizer_uk.get_vocab_size()\n",
    "\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, EN_VOCAB_SIZE, UK_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "080e110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.4738642024993895;\n",
      "Loss: 6.882545293807984;\n",
      "Loss: 6.578410665512085;\n",
      "Loss: 6.37068803691864;\n",
      "Loss: 6.210313975334167;\n",
      "Loss: 6.080472269852956;\n",
      "Loss: 5.968622493062701;\n",
      "Loss: 5.871277550458908;\n",
      "Loss: 5.7835335768593685;\n",
      "First epoch - 4.869278980255127, saving model..\n",
      "Epoch: 1, Train loss: 5.744, Val loss: 4.869,            Epoch time=445.096s\n",
      "Loss: 4.917781402587891;\n",
      "Loss: 4.884084643363953;\n",
      "Loss: 4.847352036794026;\n",
      "Loss: 4.814586849451065;\n",
      "Loss: 4.781755893135071;\n",
      "Loss: 4.750508355140686;\n",
      "Loss: 4.719063627924238;\n",
      "Loss: 4.6903286854028705;\n",
      "Loss: 4.661846391465929;\n",
      "Improved from 4.869278980255127 to 4.219856056213379, saving model..\n",
      "Epoch: 2, Train loss: 4.647, Val loss: 4.220,            Epoch time=440.598s\n",
      "Loss: 4.296400611400604;\n",
      "Loss: 4.272436888694763;\n",
      "Loss: 4.253441939671834;\n",
      "Loss: 4.235538808345795;\n",
      "Loss: 4.217567175769806;\n",
      "Loss: 4.2003291554450986;\n",
      "Loss: 4.182093508516039;\n",
      "Loss: 4.162939817488193;\n",
      "Loss: 4.146344013002183;\n",
      "Improved from 4.219856056213379 to 3.8154521627426146, saving model..\n",
      "Epoch: 3, Train loss: 4.137, Val loss: 3.815,            Epoch time=436.780s\n",
      "Loss: 3.8742926197052;\n",
      "Loss: 3.866785773277283;\n",
      "Loss: 3.8559767169952393;\n",
      "Loss: 3.8462398324012756;\n",
      "Loss: 3.8371535559654237;\n",
      "Loss: 3.8250387681325275;\n",
      "Loss: 3.814890536240169;\n",
      "Loss: 3.8050827488303183;\n",
      "Loss: 3.793548611534966;\n",
      "Improved from 3.8154521627426146 to 3.531347318649292, saving model..\n",
      "Epoch: 4, Train loss: 3.788, Val loss: 3.531,            Epoch time=435.883s\n",
      "Loss: 3.586176087856293;\n",
      "Loss: 3.5817571535110475;\n",
      "Loss: 3.5743185455004376;\n",
      "Loss: 3.568926269650459;\n",
      "Loss: 3.563326999568939;\n",
      "Loss: 3.5573836596806845;\n",
      "Loss: 3.5510175793511527;\n",
      "Loss: 3.544795946538448;\n",
      "Loss: 3.538868125120799;\n",
      "Improved from 3.531347318649292 to 3.33970454120636, saving model..\n",
      "Epoch: 5, Train loss: 3.536, Val loss: 3.340,            Epoch time=438.361s\n",
      "Loss: 3.36776177072525;\n",
      "Loss: 3.364607098340988;\n",
      "Loss: 3.3660238437652588;\n",
      "Loss: 3.3671851888895037;\n",
      "Loss: 3.3645197348594666;\n",
      "Loss: 3.361388240257899;\n",
      "Loss: 3.3567666476113454;\n",
      "Loss: 3.353936026453972;\n",
      "Loss: 3.3491689920425416;\n",
      "Improved from 3.33970454120636 to 3.198787411689758, saving model..\n",
      "Epoch: 6, Train loss: 3.348, Val loss: 3.199,            Epoch time=437.519s\n",
      "Loss: 3.21491307926178;\n",
      "Loss: 3.211948087930679;\n",
      "Loss: 3.212973178863525;\n",
      "Loss: 3.2117213805913924;\n",
      "Loss: 3.2109116906166077;\n",
      "Loss: 3.2080261651674906;\n",
      "Loss: 3.2068325382641385;\n",
      "Loss: 3.206605151176453;\n",
      "Loss: 3.2048884885046216;\n",
      "Improved from 3.198787411689758 to 3.096737716674805, saving model..\n",
      "Epoch: 7, Train loss: 3.204, Val loss: 3.097,            Epoch time=436.504s\n",
      "Loss: 3.080478078365326;\n",
      "Loss: 3.08159996509552;\n",
      "Loss: 3.084718329747518;\n",
      "Loss: 3.087246499419212;\n",
      "Loss: 3.089294595146179;\n",
      "Loss: 3.0906103352705636;\n",
      "Loss: 3.0923926604134695;\n",
      "Loss: 3.092711854994297;\n",
      "Loss: 3.09272241836124;\n",
      "Improved from 3.096737716674805 to 3.018724002838135, saving model..\n",
      "Epoch: 8, Train loss: 3.092, Val loss: 3.019,            Epoch time=435.545s\n",
      "Loss: 2.981190782546997;\n",
      "Loss: 2.985736932039261;\n",
      "Loss: 2.9873152136802674;\n",
      "Loss: 2.990326759934425;\n",
      "Loss: 2.992565347290039;\n",
      "Loss: 2.9954203051726025;\n",
      "Loss: 2.9969959803989954;\n",
      "Loss: 2.9992390491962433;\n",
      "Loss: 3.0003933587604097;\n",
      "Improved from 3.018724002838135 to 2.9566074361801147, saving model..\n",
      "Epoch: 9, Train loss: 3.001, Val loss: 2.957,            Epoch time=437.261s\n",
      "Loss: 2.896403172492981;\n",
      "Loss: 2.9028795511722563;\n",
      "Loss: 2.908806289990743;\n",
      "Loss: 2.9146809405088425;\n",
      "Loss: 2.9183496700286864;\n",
      "Loss: 2.9214602165222168;\n",
      "Loss: 2.9238459771701266;\n",
      "Loss: 2.9262469201683996;\n",
      "Loss: 2.9267581794526842;\n",
      "Improved from 2.9566074361801147 to 2.9074125652313234, saving model..\n",
      "Epoch: 10, Train loss: 2.928, Val loss: 2.907,            Epoch time=434.303s\n",
      "Loss: 2.822809928894043;\n",
      "Loss: 2.8282524077892304;\n",
      "Loss: 2.835663449605306;\n",
      "Loss: 2.8420167179107665;\n",
      "Loss: 2.848409708404541;\n",
      "Loss: 2.8527893127600352;\n",
      "Loss: 2.8560132717405047;\n",
      "Loss: 2.861510855793953;\n",
      "Loss: 2.8641792119873895;\n",
      "Improved from 2.9074125652313234 to 2.8647556200027466, saving model..\n",
      "Epoch: 11, Train loss: 2.865, Val loss: 2.865,            Epoch time=434.493s\n",
      "Loss: 2.775860308170319;\n",
      "Loss: 2.7814550466537478;\n",
      "Loss: 2.7849305346806843;\n",
      "Loss: 2.790383629441261;\n",
      "Loss: 2.793429064941406;\n",
      "Loss: 2.7973027313550314;\n",
      "Loss: 2.801498835359301;\n",
      "Loss: 2.8050704925060272;\n",
      "Loss: 2.8076158651245966;\n",
      "Improved from 2.8647556200027466 to 2.8326815671920778, saving model..\n",
      "Epoch: 12, Train loss: 2.809, Val loss: 2.833,            Epoch time=436.773s\n",
      "Loss: 2.7226097655296324;\n",
      "Loss: 2.729508892774582;\n",
      "Loss: 2.7326206375757853;\n",
      "Loss: 2.7380094703435898;\n",
      "Loss: 2.7435491309165956;\n",
      "Loss: 2.7487988228797913;\n",
      "Loss: 2.7531498299326214;\n",
      "Loss: 2.7560253889560697;\n",
      "Loss: 2.7587883259455364;\n",
      "Improved from 2.8326815671920778 to 2.805587452888489, saving model..\n",
      "Epoch: 13, Train loss: 2.761, Val loss: 2.806,            Epoch time=436.557s\n",
      "Loss: 2.6654920244216918;\n",
      "Loss: 2.6758711907863617;\n",
      "Loss: 2.6861210487683613;\n",
      "Loss: 2.6931229701042176;\n",
      "Loss: 2.7002967206954955;\n",
      "Loss: 2.705863682905833;\n",
      "Loss: 2.711187966823578;\n",
      "Loss: 2.7154903321266173;\n",
      "Loss: 2.718806847360399;\n",
      "Improved from 2.805587452888489 to 2.7770236711502077, saving model..\n",
      "Epoch: 14, Train loss: 2.721, Val loss: 2.777,            Epoch time=434.595s\n",
      "Loss: 2.623921781539917;\n",
      "Loss: 2.639586383342743;\n",
      "Loss: 2.6484369109471637;\n",
      "Loss: 2.6560717899799346;\n",
      "Loss: 2.661147226238251;\n",
      "Loss: 2.6670627261002857;\n",
      "Loss: 2.672048565728324;\n",
      "Loss: 2.676784774839878;\n",
      "Loss: 2.6807646323310004;\n",
      "Improved from 2.7770236711502077 to 2.7564092416763306, saving model..\n",
      "Epoch: 15, Train loss: 2.683, Val loss: 2.756,            Epoch time=437.593s\n",
      "Loss: 2.5970134797096254;\n",
      "Loss: 2.6074364912509918;\n",
      "Loss: 2.6163910659154257;\n",
      "Loss: 2.623137573480606;\n",
      "Loss: 2.628703873157501;\n",
      "Loss: 2.6352565150260925;\n",
      "Loss: 2.640308726991926;\n",
      "Loss: 2.64463726824522;\n",
      "Loss: 2.649231708844503;\n",
      "Improved from 2.7564092416763306 to 2.741998230934143, saving model..\n",
      "Epoch: 16, Train loss: 2.651, Val loss: 2.742,            Epoch time=434.402s\n",
      "Loss: 2.5657219767570494;\n",
      "Loss: 2.577629350423813;\n",
      "Loss: 2.5853314078648886;\n",
      "Loss: 2.5931264086961745;\n",
      "Loss: 2.5983863144874575;\n",
      "Loss: 2.6045138041973113;\n",
      "Loss: 2.610237490722111;\n",
      "Loss: 2.6155567574501037;\n",
      "Loss: 2.619784315639072;\n",
      "Improved from 2.741998230934143 to 2.720949007987976, saving model..\n",
      "Epoch: 17, Train loss: 2.621, Val loss: 2.721,            Epoch time=434.240s\n",
      "Loss: 2.5311718740463256;\n",
      "Loss: 2.5460445041656494;\n",
      "Loss: 2.556397507985433;\n",
      "Loss: 2.563757224440575;\n",
      "Loss: 2.5712142266273497;\n",
      "Loss: 2.577899616400401;\n",
      "Loss: 2.583350742135729;\n",
      "Loss: 2.589074465870857;\n",
      "Loss: 2.593064450846778;\n",
      "Improved from 2.720949007987976 to 2.7089407958984375, saving model..\n",
      "Epoch: 18, Train loss: 2.595, Val loss: 2.709,            Epoch time=437.567s\n",
      "Loss: 2.508686089992523;\n",
      "Loss: 2.5249576320648193;\n",
      "Loss: 2.5332134757041933;\n",
      "Loss: 2.539799709558487;\n",
      "Loss: 2.546782395839691;\n",
      "Loss: 2.5525787014961243;\n",
      "Loss: 2.5576033539772034;\n",
      "Loss: 2.5627542642951013;\n",
      "Loss: 2.5681919553014967;\n",
      "Improved from 2.7089407958984375 to 2.6916371517181394, saving model..\n",
      "Epoch: 19, Train loss: 2.570, Val loss: 2.692,            Epoch time=435.628s\n",
      "Loss: 2.4834697489738464;\n",
      "Loss: 2.4979811532497407;\n",
      "Loss: 2.5071717958450317;\n",
      "Loss: 2.5142512630224227;\n",
      "Loss: 2.521587213516235;\n",
      "Loss: 2.5278259096145628;\n",
      "Loss: 2.534134617124285;\n",
      "Loss: 2.5396072326898573;\n",
      "Loss: 2.546215207576752;\n",
      "Improved from 2.6916371517181394 to 2.686845037460327, saving model..\n",
      "Epoch: 20, Train loss: 2.548, Val loss: 2.687,            Epoch time=436.160s\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
    "    \n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(transformer, 'model')\n",
    "    \n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(transformer, 'model')\n",
    "    \n",
    "    losses.append(val_loss)\n",
    "        \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819d81b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (generator): Linear(in_features=256, out_features=30000, bias=True)\n",
       "  (src_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(30000, 256)\n",
       "  )\n",
       "  (tgt_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(30000, 256)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c6bef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torch.load('model').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8fca62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
    "    output_ids = [tokenizer_uk.token_to_id('[CLS]')]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
    "\n",
    "    (texts_en_mask, texts_uk_mask, \n",
    "    texts_en_padding_mask, texts_uk_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_uk_mask,\n",
    "                   texts_en_padding_mask, texts_uk_padding_mask, texts_en_padding_mask)\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_uk.token_to_id('[SEP]'), tokenizer_uk.token_to_id('[PAD]')]:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
    "\n",
    "        (texts_en_mask, texts_uk_mask, \n",
    "        texts_en_padding_mask, texts_uk_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_uk_mask,\n",
    "                       texts_en_padding_mask, texts_uk_padding_mask, texts_en_padding_mask)\n",
    "        pred = logits.argmax(2)[-1].item()\n",
    "\n",
    "    return (' '.join([tokenizer_uk.id_to_token(i).replace('##', '') for i in output_ids[1:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838d2ab",
   "metadata": {},
   "source": [
    "На всех тестировать я все же не стал, особо смысла мне кажется не имеет, но результат хороший в любом случае:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0150a1c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  Consistant with our other victims.\n",
      "Ukrainian:  И все равно , что с нашими другими жерт вами .\n",
      "\n",
      "English:  The children.\n",
      "Ukrainian:  Діти .\n",
      "\n",
      "English:  I still have some friends in high places.\n",
      "Ukrainian:  У меня еще есть друзья в высо ких местах .\n",
      "\n",
      "English:  Anything else you can tell us?\n",
      "Ukrainian:  Что - нибудь ещё ты можешь рассказать нам ?\n",
      "\n",
      "English:  Successfully moved to trash.\n",
      "Ukrainian:  Успі шно пересунуто до смітника .\n",
      "\n",
      "English:  The given name could not be resolved to a unique server. Make sure your network is setup without any name conflicts between names used by Windows and by UNIX name resolution.\n",
      "Ukrainian:  Не вдалося визначити назву , який буде використано уніка льну сервер . Переконайтеся , що вашу мережу не буде використано для будь - якого з назв конфліктів , які використовуються у Windows і UNIX .\n",
      "\n",
      "English:  Jake, your mom's on the phone!\n",
      "Ukrainian:  Джейк , твоя мати на телефо ні !\n",
      "\n",
      "English:  Father, no!\n",
      "Ukrainian:  Батьку , ні !\n",
      "\n",
      "English:  Data tool\n",
      "Ukrainian:  Інструмент даних\n",
      "\n",
      "English:  Shh, shh, shh, shh.\n",
      "Ukrainian:  шш шш шш шш шш шш шш шш .\n",
      "\n",
      "English:  Close the door.\n",
      "Ukrainian:  Закри йте двері .\n",
      "\n",
      "English:  Everything gonna be okay.\n",
      "Ukrainian:  Все будет хорошо .\n",
      "\n",
      "English:  I'm just glad he's feeling better.\n",
      "Ukrainian:  Я рада , что он себя чувствую .\n",
      "\n",
      "English:  Don't you touch her!\n",
      "Ukrainian:  Не трогай её !\n",
      "\n",
      "English:  So?\n",
      "Ukrainian:  Ну ?\n",
      "\n",
      "English:  Subdued\n",
      "Ukrainian:  Під ду шення\n",
      "\n",
      "English:  That's why he gave me a code.\n",
      "Ukrainian:  Поэтому он дал мне код .\n",
      "\n",
      "English:  It's lonely in Neverland.\n",
      "Ukrainian:  Это одиноко в Не вер ленде .\n",
      "\n",
      "English:  Notes\n",
      "Ukrainian:  Нотатки\n",
      "\n",
      "English:  Look, he's my lawyer, same as yours.\n",
      "Ukrainian:  Слушай , он мой адвокат , и твой .\n",
      "\n",
      "English:  Forget about control, Nina. I want to see passion.\n",
      "Ukrainian:  Забудь про контроль , Ніно я хочу побачити пристра сть .\n",
      "\n",
      "English:  I pray you two see each other again soon.\n",
      "Ukrainian:  Я мо лю лю тебе , як тільки одного разу .\n",
      "\n",
      "English:  Baby, she's a material witness.\n",
      "Ukrainian:  Детка , она свидетель ница .\n",
      "\n",
      "English:  Configure the number of calendar days you wish to be published and available to others\n",
      "Ukrainian:  Налаштування кількості днів , який ви бажаєте оприлюд нити і доступ до інших\n",
      "\n",
      "English:  The shader should run!\n",
      "Ukrainian:  The F ri er s to run ning !\n",
      "\n",
      "English:  Any Video4Linux compatible device\n",
      "Ukrainian:  Будь - який суміс ний з відео - 4 Linux\n",
      "\n",
      "English:  How did you end up here?\n",
      "Ukrainian:  Как ты сюда попала ?\n",
      "\n",
      "English:  - What do you expect?\n",
      "Ukrainian:  - Що ти очіку єш ?\n",
      "\n",
      "English:  - Okay.\n",
      "Ukrainian:  - Хорошо .\n",
      "\n",
      "English:  Mmm...\n",
      "Ukrainian:  Ммм ...\n",
      "\n",
      "English:  Top Output Bin (Face Down)\n",
      "Ukrainian:  Верхній вивід ний ( лицем вниз )\n",
      "\n",
      "English:  Well, you know, I...\n",
      "Ukrainian:  Ну , знаешь , я ...\n",
      "\n",
      "English:  Mouse keys\n",
      "Ukrainian:  клавіші мишки\n",
      "\n",
      "English:  -Hey.\n",
      "Ukrainian:  - Привет .\n",
      "\n",
      "English:  You have to enter the name of the new identity into the New Identity edit field. This will be the name shown in the identity list.\n",
      "Ukrainian:  Вам слід ввести назву нового профілю у поле Новий профіль . Це буде показано назву у списку профілів .\n",
      "\n",
      "English:  No.\n",
      "Ukrainian:  Ні .\n",
      "\n",
      "English:  You're confusing me.\n",
      "Ukrainian:  Ты меня запу тан ешь .\n",
      "\n",
      "English:  Leo: Come on, look how cute they are.\n",
      "Ukrainian:  Да ладно , посмотри как они милые .\n",
      "\n",
      "English:  Fuck.\n",
      "Ukrainian:  Блядь .\n",
      "\n",
      "English:  Because you are a good man.\n",
      "Ukrainian:  Потому что ты хороший человек .\n",
      "\n",
      "English:  Maybe she'll let us borrow a dress too.\n",
      "Ukrainian:  Может , она позво нит нам платье ?\n",
      "\n",
      "English:  A show of unity, so to speak.\n",
      "Ukrainian:  Про покажу вам є є ння .\n",
      "\n",
      "English:  Sharp turns\n",
      "Ukrainian:  Шар п сія\n",
      "\n",
      "English:  How do you know that Amy's the head cheerleader?\n",
      "Ukrainian:  Откуда ты знаешь , что Эми - голова Эми ?\n",
      "\n",
      "English:  Good job.\n",
      "Ukrainian:  Хорошая работа .\n",
      "\n",
      "English:  And, Oaks, no one else comes in.\n",
      "Ukrainian:  І О кс , ніхто інший не прийде .\n",
      "\n",
      "English:  I'll work something out.\n",
      "Ukrainian:  Я щось з ' ясу ю .\n",
      "\n",
      "English:  Hey, fuck you, Kyle!\n",
      "Ukrainian:  Эй , блядь , ты , Кайл !\n",
      "\n",
      "English:  Okay, I'll admit that's worth getting on the \"G\" train for.\n",
      "Ukrainian:  Хорошо , я призна юсь , что стоит на поезд \" Га \"\n",
      "\n",
      "English:  And how many years did it take to build-- my grandfather and Uncle were murdered here, weren't they?\n",
      "Ukrainian:  І скільки років це взяв , щоб побудувати ... мій дід і дядько був вби тий тут ?\n",
      "\n",
      "English:  I want you to steal the diamonds. - No, that's...\n",
      "Ukrainian:  Я хочу , чтобы ты украл алма зы .\n",
      "\n",
      "English:  You are so very much alike.\n",
      "Ukrainian:  Ти такий самий .\n",
      "\n",
      "English:  Flanks up!\n",
      "Ukrainian:  - З кі бер кі !\n",
      "\n",
      "English:  Good, 'cause those boxes Aren't gonna deliver colors.\n",
      "Ukrainian:  Хорошо , потому что эти ящи ки не доста влять кольорів .\n",
      "\n",
      "English:  She can speak not only English but also French.\n",
      "Ukrainian:  Вона не може говорити не лише англійською , але також французь кі .\n",
      "\n",
      "English:  Oh I told him...\n",
      "Ukrainian:  Я сказала ему ...\n",
      "\n",
      "English:  I'm waiting for word on an assignment that will take me far away from the capital.\n",
      "Ukrainian:  Я чекаю на слово на призначення , що від мене від столи ця від столи ця .\n",
      "\n",
      "English:  Look at these women.\n",
      "Ukrainian:  Посмотри на этих женщин .\n",
      "\n",
      "English:  Delete the current profile\n",
      "Ukrainian:  Вилучити поточний профіль\n",
      "\n",
      "English:  It's an Earth-shaking discovery, and it's forcing us to radically reassess our place in the Universe and even our eventual fate.\n",
      "Ukrainian:  Це відкриття Землі і він примушу є нас раді сти тися над нашим місцем в всесвіті і навіть нашу долю .\n",
      "\n",
      "English:  - It's great.\n",
      "Ukrainian:  - Чудово .\n",
      "\n",
      "English:  I'm not going back.\n",
      "Ukrainian:  Я не повернуся .\n",
      "\n",
      "English:  Pissy little girlie smokes.\n",
      "Ukrainian:  Пи ши шка па ет ку рю .\n",
      "\n",
      "English:  You can't... you...\n",
      "Ukrainian:  Ты не можешь ... ты ...\n",
      "\n",
      "English:  Whoa, excuse me.\n",
      "Ukrainian:  Простите .\n",
      "\n",
      "English:  Okay, you really gotta help me out here.\n",
      "Ukrainian:  Ладно , ты действительно должен мне помочь .\n",
      "\n",
      "English:  Hit me.\n",
      "Ukrainian:  В дар мене .\n",
      "\n",
      "English:  We can shake hands with Russian guests and wish them a safe return home.\n",
      "Ukrainian:  Ми можемо тря сти руки з російсь ким го гості і хочемо , щоб вони поверну ли додому .\n",
      "\n",
      "English:  The SUMSQ() function calculates the sum of all the squares of values given as parameters. You can calculate the sum of a range SUMSQ(A1: B5) or a list of values like SUMSQ(12; 5; 12.5).\n",
      "Ukrainian:  Функція SU M SQ () обчис лює суму всіх квадра тів значень , які буде використано як параметри . Ви можете визначити суму діапазону значень , наприклад , M B M DI ( A1 : B 5 ) або списком значень SU M SQ ( 12 ; 12 ; 12\n",
      "\n",
      "English:  We have had enough.\n",
      "Ukrainian:  У нас было достаточно .\n",
      "\n",
      "English:  Because General Donovan ordered it, that's why.\n",
      "Ukrainian:  Потому что генерал Донован это заказы вал , вот почему .\n",
      "\n",
      "English:  1971?\n",
      "Ukrainian:  197 1 рік ?\n",
      "\n",
      "English:  Alice, don't let him do this.\n",
      "Ukrainian:  Элис , не дай ему это сделать .\n",
      "\n",
      "English:  So is there anything I should know about?\n",
      "Ukrainian:  Так что я должен знать о чем - то ?\n",
      "\n",
      "English:  Hey, Brandon!\n",
      "Ukrainian:  Эй , Брэндон !\n",
      "\n",
      "English:  I keep paging cardio and expecting Cristina to just walk through the door.\n",
      "Ukrainian:  Я продол жаю иметь серде чный кар дия и ожи даю , что Кри стина в дверь .\n",
      "\n",
      "English:  What about Scott?\n",
      "Ukrainian:  А что насчет Скотт ?\n",
      "\n",
      "English:  You were responsible.\n",
      "Ukrainian:  Ты была ответствен ным .\n",
      "\n",
      "English:  Light Snow Drizzle\n",
      "Ukrainian:  Світло - Сноу снігweather condition\n",
      "\n",
      "English:  We did.\n",
      "Ukrainian:  Ми зробили .\n",
      "\n",
      "English:  May\n",
      "Ukrainian:  Май\n",
      "\n",
      "English:  Oh, what the hell is this?\n",
      "Ukrainian:  Что это за черт ?\n",
      "\n",
      "English:  It's the last thing he'll expect.\n",
      "Ukrainian:  Это последний , что он ожи дает .\n",
      "\n",
      "English:  All right.\n",
      "Ukrainian:  Гаразд .\n",
      "\n",
      "English:  -What's the matter?\n",
      "Ukrainian:  - Що сталося ?\n",
      "\n",
      "English:  Are you OK?\n",
      "Ukrainian:  Ты в порядке ?\n",
      "\n",
      "English:  - Maybe.\n",
      "Ukrainian:  - Може .\n",
      "\n",
      "English:  But one thing's certain.\n",
      "Ukrainian:  Но одна вещь .\n",
      "\n",
      "English:  How much did they cost?\n",
      "Ukrainian:  Сколько они сто или ?\n",
      "\n",
      "English:  Hey, come here.\n",
      "Ukrainian:  Эй , иди сюда .\n",
      "\n",
      "English:  This is Debbie!\n",
      "Ukrainian:  Это Деб бі !\n",
      "\n",
      "English:  Boom.\n",
      "Ukrainian:  Бум .\n",
      "\n",
      "English:  Where were you two days ago?\n",
      "Ukrainian:  Где ты был два дня назад ?\n",
      "\n",
      "English:  So does Calista.\n",
      "Ukrainian:  Так Ка листа .\n",
      "\n",
      "English:  I'm sorry.\n",
      "Ukrainian:  Мне жаль .\n",
      "\n",
      "English:  Konqueror is a combined web- and filebrowser.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukrainian:  Konqueror є разом з веб - і переглядача файлів .\n",
      "\n",
      "English:  Delete the selected task.\n",
      "Ukrainian:  Вилучити позначений завдання .\n",
      "\n",
      "English:  %1%\n",
      "Ukrainian:  % 1 %\n",
      "\n",
      "English:  - Mm-hmm.\n",
      "Ukrainian:  - Угу .\n",
      "\n",
      "English:  (sighs) I just...\n",
      "Ukrainian:  Я просто ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in en_sents[:100]:\n",
    "    print('English: ', sent)\n",
    "    print('Ukrainian: ', translate(sent))\n",
    "    print( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c387fbb",
   "metadata": {},
   "source": [
    "Сделаем функцию для генерации батчами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "f246b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_texts(list_of_texts):\n",
    "    gen_texts = {}\n",
    "    input_ids = [[tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')] for text in list_of_texts]\n",
    "    output_ids = [[tokenizer_uk.token_to_id('[CLS]')] for _ in range(len(list_of_texts))]\n",
    "\n",
    "    input_ids = [torch.LongTensor(ids) for ids in input_ids]\n",
    "    output_ids = [torch.LongTensor(ids) for ids in output_ids]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence(input_ids)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence(output_ids)\n",
    "\n",
    "    (texts_en_mask, texts_ru_mask, texts_en_padding_mask, texts_ru_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_ru_mask,\n",
    "                       texts_en_padding_mask, texts_ru_padding_mask, texts_en_padding_mask)\n",
    "\n",
    "    pred = logits.argmax(2)\n",
    "    output_ids_pad = torch.cat([output_ids_pad, pred])\n",
    "\n",
    "    for _ in tqdm(range(20)):\n",
    "        (texts_en_mask, texts_uk_mask, texts_en_padding_mask, texts_uk_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
    "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_uk_mask,\n",
    "                                   texts_en_padding_mask, texts_uk_padding_mask, texts_en_padding_mask)\n",
    "        pred = logits.argmax(2)[-1].unsqueeze(0)\n",
    "\n",
    "        if tokenizer_uk.token_to_id('[SEP]') in pred or tokenizer_uk.token_to_id('[PAD]') in pred:\n",
    "            pr = pred.squeeze(0)\n",
    "            leave_ids = np.where((pr == tokenizer_uk.token_to_id('[SEP]')) | (pr == tokenizer_uk.token_to_id('[PAD]')))[0]\n",
    "            leave_output_ids = output_ids_pad[:, [id_ for id_ in range(output_ids_pad.shape[1]) if id_ in leave_ids]]\n",
    "\n",
    "            for i, id_ in enumerate(leave_ids):\n",
    "                gen_text = ' '.join([tokenizer_uk.id_to_token(j) for j in leave_output_ids[:, i]])\n",
    "                gen_texts[id_] = gen_text\n",
    "\n",
    "            input_ids_pad = input_ids_pad[:, [id_ for id_ in range(input_ids_pad.shape[1]) if id_ not in leave_ids]]\n",
    "            output_ids_pad = output_ids_pad[:, [id_ for id_ in range(output_ids_pad.shape[1]) if id_ not in leave_ids]]\n",
    "            pred = pred[:, [id_ for id_ in range(pred.shape[1]) if id_ not in leave_ids]]\n",
    "        output_ids_pad = torch.cat([output_ids_pad, pred])\n",
    "\n",
    "    for i in range(output_ids_pad.shape[1]):\n",
    "        if i in gen_texts.keys():\n",
    "            continue\n",
    "        else:\n",
    "            gen_text = ' '.join([tokenizer_uk.id_to_token(j) for j in output_ids_pad[:, i]])\n",
    "            gen_texts[i] = gen_text\n",
    "    gen_texts = dict(sorted(gen_texts.items(), key=lambda x: x[0]))\n",
    "    gen_texts = list(gen_texts.values())\n",
    "    return gen_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ccde0de7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 13.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "inds = random.choices(range(5, 1000), k=50)\n",
    "orig_en_texts = np.array(en_sents[:1000])[inds]\n",
    "\n",
    "translated_uk_texts = translate_texts(orig_en_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "7112da2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  Hook me up?\n",
      "Ukrainian:  [CLS] У ко пу шки , у нас є можливість , що у вас є пу шки , у грі є ко\n",
      "\n",
      "English:  - You busy?\n",
      "Ukrainian:  [CLS] У нас є повно му , що ви знайдете у грі , у грі є ко пу ків , що ви\n",
      "\n",
      "English:  You got them going out there, Mike.\n",
      "Ukrainian:  [CLS] У вас є їх там , Майк , у нас є всі єю програмою , що у вас є повно му\n",
      "\n",
      "English:  It's your father.\n",
      "Ukrainian:  [CLS] У нас є ваш батько , і у грі є ко пу ків , що гри ми , вті хи ,\n",
      "\n",
      "English:  The SERIESSUM() function returns the sum of a power series.\n",
      "Ukrainian:  [CLS] Функція S ER I ES SU M () повертає суму з використанням ряд ження м , що програма для гри у\n",
      "\n",
      "English:  This dialog will allow you to specify a PostgreSQL account that has the necessary permissions to access the Krecipes PostgreSQL database. This account may either be a PostgreSQL superuser or have the ability to both create new PostgreSQL users and databases. If no superuser or privileged account is given, the account'postgres' will be attempted, with no password. If this is insufficient for your PostgreSQL setup, you must select the appropriate option below to enter the information of a privileged PostgreSQL account.\n",
      "Ukrainian:  [CLS] За допомогою цього діалогового вікна ви можете вказати обліковий запис Post gre SQL , який має потрібні права доступу до бази\n",
      "\n",
      "English:  - No, it's not.\n",
      "Ukrainian:  [CLS] У мене є повно му , у якому ви знайдете всі єю програмою , у якому ви знайдете всі єю програмою\n",
      "\n",
      "English:  You see, Tom, my dad, who drank himself to death, along with your dad and Ben here, decided to take the law into their own hands ten years ago.\n",
      "Ukrainian:  [CLS] Том , мой папа , который выпи ла себя до смерти , вместе с твоим отцом и Бен ком решили принять\n",
      "\n",
      "English:  In time, I fear he will negotiate with my father, unraveling all you and I have built.\n",
      "Ukrainian:  [CLS] У час , боюсь , що він вести переговори з моїм батьком , роз тяг ну всі всі ваші і я\n",
      "\n",
      "English:  Jesus Christ, there's kids in there, sir!\n",
      "Ukrainian:  [CLS] Господи Иисусе , дети у них , сэр ! У нас есть вещи , у вас есть пре бы вание пу\n",
      "\n",
      "English:  Demultiplexer used for the file or stream\n",
      "Ukrainian:  [CLS] На ро дні , які використовуються для файла або потоку даних , у якому ви знайдете всі єю програмою , що\n",
      "\n",
      "English:  Copy to repository\n",
      "Ukrainian:  [CLS] Копіювати до сховища ( місто гри у K od u ad o od z o od u les G od al\n",
      "\n",
      "English:  Are we ever gonna go on a normal date together?\n",
      "Ukrainian:  [CLS] Ми коли - небудь підемо на норма льні дати і всі єю програмою K od u ad o od u z\n",
      "\n",
      "English:  Well, I wrangled for a while, and then they saw I could say a line or two, and I was Bad Clem or Deputy Number Two or a guy's buddy for a couple of years, and then someone heard me sing and they made me the guy.\n",
      "Ukrainian:  [CLS] Ну , я спо ру чив за деякий час , а потім побачив , що я міг би сказати слово або\n",
      "\n",
      "English:  Valletta\n",
      "Ukrainian:  [CLS] У всіх най ниж чих ко пу хо ків , у яких є го ви щу вання у грі хо ваються\n",
      "\n",
      "English:  Sorry, sirs.\n",
      "Ukrainian:  [CLS] Вибачте , сер у K od u a ch il le ag u a K od u les G od u\n",
      "\n",
      "English:  How much did they cost?\n",
      "Ukrainian:  [CLS] У нас є багато років , у якому ви знайдете всі єю програмою , у якому ви знайдете всі єю програмою\n",
      "\n",
      "English:  You gave me all that time off, and I really should've-\n",
      "Ukrainian:  [CLS] Ви дали мені всі ті , що разу , і я дійсно повинен мати змогу отримати всі ці ла пу шки\n",
      "\n",
      "English:  Why don't you just shut up and take your own damned advice?\n",
      "Ukrainian:  [CLS] Чому б вам не просто закри вати і не закри йте свій свій прокля т вір ко пу ків ку пі\n",
      "\n",
      "English:  Well, never assume anything about me.\n",
      "Ukrainian:  [CLS] що ж , ніколи не припуска є , що що що завгодно . У цьому ро щі льні зави вання з\n",
      "\n",
      "English:  Why aren't the guys putting out the fires?\n",
      "Ukrainian:  [CLS] Чому хлопці не ви кла дуть вогонь до вогню з ко пу ків , що ви знайдете у грі хо ві\n",
      "\n",
      "English:  But how did you know?\n",
      "Ukrainian:  [CLS] У нас є пу шки , у яких ви знаєте , у нас є всі єю адресою , що ви знайдете\n",
      "\n",
      "English:  Raise\n",
      "Ukrainian:  [CLS] У па пі чні мо дер і від гри у K od om al g un es o od z o\n",
      "\n",
      "English:  Great.\n",
      "Ukrainian:  [CLS] У & # 160 ; гри у K od u ff le H u les G od u les cript ive\n",
      "\n",
      "English:  And I know I saw a flash of her yesterday.\n",
      "Ukrainian:  [CLS] Я знаю , що я бачив спа н з її вчора , і у них є можливість , що у них\n",
      "\n",
      "English:  That's a big ask.\n",
      "Ukrainian:  [CLS] У цьому підручнику ви знайдете багато ро жні х ла мати ків , що у вас є повно му ро жні\n",
      "\n",
      "English:  File Tree\n",
      "Ukrainian:  [CLS] У програмі передбачено можливість створення файлів , у якому ви знайдете всі єю програмою , у якому ви знайдете всі єю\n",
      "\n",
      "English:  Oh, what the hell is this?\n",
      "Ukrainian:  [CLS] У нас є повно му ро жні х ла пу ків , що у вас є пі ко пу ків ,\n",
      "\n",
      "English:  He's adjusting.\n",
      "Ukrainian:  [CLS] У нього є спосіб , у якому ви знайдете всі єю програмою , у якому ви знайдете всі єю програмою ,\n",
      "\n",
      "English:  Minimal View\n",
      "Ukrainian:  [CLS] Міні мальний перегляд інструментів ( ис п ’ я ниці з гри ми ), у якому ви знайдете всі єю програмою\n",
      "\n",
      "English:  - No, it's not.\n",
      "Ukrainian:  [CLS] У мене є повно му , у якому ви знайдете всі єю програмою , у якому ви знайдете всі єю програмою\n",
      "\n",
      "English:  This is Debbie!\n",
      "Ukrainian:  [CLS] Це бар ду , що ви побачите , у нас є всі єю програмою , у якому ви знайдете всі єю\n",
      "\n",
      "English:  Created thumbnail for: %1\n",
      "Ukrainian:  [CLS] С творено мініатюр для : « Ко смі чна програма для створення гри у K od u ff le g od\n",
      "\n",
      "English:  And you must lay so very nice and still.\n",
      "Ukrainian:  [CLS] І ви повинні бути дуже гарно і дуже гарно , і все одно , що ви жили у грі хо ві\n",
      "\n",
      "English:  You think it's fun?\n",
      "Ukrainian:  [CLS] Ви вважаєте , що це весело щі , у нас є всі єю програмою , що у вас є пі ко\n",
      "\n",
      "English:  What do you mean there is more than one application per & MIME; type? Why is this necessary?\n",
      "Ukrainian:  [CLS] Що означає , що у вас є декілька програм для & MIME ; ? Для цього слід виконати пошук у програмі\n",
      "\n",
      "English:  Oh, I'm sorry.\n",
      "Ukrainian:  [CLS] О , мені шкода , що у вас є ла пу шки , у нас є всі єю пу шки ,\n",
      "\n",
      "English:  Yes, it still stands at 74%.\n",
      "Ukrainian:  [CLS] Так , все одно , це все ще є у 7 4 %. У від пу щен я з ’ єднання\n",
      "\n",
      "English:  Configure the number of calendar days you wish to be published and available to others\n",
      "Ukrainian:  [CLS] Тут ви можете налаштувати кількість днів , які ви бажаєте оприлюд нити і доступ до інших програм , які ви побачите\n",
      "\n",
      "English:  Ohhh.\n",
      "Ukrainian:  [CLS] О , ко пу шки , у нас є всі єю адресою , K od u les G od z o\n",
      "\n",
      "English:  In time, I fear he will negotiate with my father, unraveling all you and I have built.\n",
      "Ukrainian:  [CLS] У час , боюсь , що він вести переговори з моїм батьком , роз тяг ну всі всі ваші і я\n",
      "\n",
      "English:  What do you mean there is more than one application per & MIME; type? Why is this necessary?\n",
      "Ukrainian:  [CLS] Що означає , що у вас є декілька програм для & MIME ; ? Для цього слід виконати пошук у програмі\n",
      "\n",
      "English:  Media Type First\n",
      "Ukrainian:  [CLS] У першому з типів даних , у яких ви знайдете всі єю програмою , у якому ви знайдете всі єю програмою\n",
      "\n",
      "English:  If you relax, it will enable me to do anything I please\n",
      "Ukrainian:  [CLS] Якщо ви розсла битися , то буде увімкнути або вимкнути всі єю програмою , будь ласка , у грі є K\n",
      "\n",
      "English:  Go, find the cure!\n",
      "Ukrainian:  [CLS] Щоб отримати всі єю адресою , ви можете створити цю дію , у грі є K od u ad o od\n",
      "\n",
      "English:  Between the breakup, and then her acting career isn't going that well...\n",
      "Ukrainian:  [CLS] Между расста вания и потом её карь ера не будет , что хорошо , что у нас есть ис хо ль\n",
      "\n",
      "English:  - 8.5 for 10!\n",
      "Ukrainian:  [CLS] У грі є ла пу шки , у нас є всі єю , що ви знайдете тут , у грі є\n",
      "\n",
      "English:  That means I will.\n",
      "Ukrainian:  [CLS] У нас є повно му ро жні х ла пу ків , у якому ви знайдете всі єю програмою , що\n",
      "\n",
      "English:  Somewhere far away from there.\n",
      "Ukrainian:  [CLS] Десь далеко від них , у них є го ла пу ків , і у грі є ко пу ків ,\n",
      "\n",
      "English:  KEuroCalc\n",
      "Ukrainian:  [CLS] K Євро o Cal c Cal c ul se k br uch e al g un es o od u a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en_sent, uk_sent in zip(orig_en_texts, translated_uk_texts):\n",
    "    print('English: ', en_sent)\n",
    "    print('Ukrainian: ', uk_sent)\n",
    "    print( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312018ea",
   "metadata": {},
   "source": [
    "В общем почти все ок, за исключением того, что [SEP] токен почему-то генерируется не во всех случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe83a9a",
   "metadata": {},
   "source": [
    "\n",
    "## Задание 2 (2 балла).\n",
    "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
    "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529c3f4",
   "metadata": {},
   "source": [
    "Данная техника используется для обучения модели машинного перевода в том случае, если у нас мало данных таргетного языка, того на который мы хотим переводить. <br>\n",
    "\n",
    "Например, у нас есть параллельный корпус en-ru в 10000 предложений, и дополнительный корпус en в 100000 предложений, и мы хотим обучить модель машинного перевода ru-en. Мы берем и обучаем модель машиннного перевода с en на ru на 10000 предложениях. После чего генерируем с помощью данной модели перевод 100000 предложений en из дополнительного корпуса. После чего складываем наш исходных параллельный корпус со сгенерированным корпусом 10000 + 100000 = 110000. После чего уже обучаем еще одну модель для перевода с ru на en. Авторы статьи пишут, что такая модель показывает 2/3 результативности модели, обученной в обычном режиме, без back translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
